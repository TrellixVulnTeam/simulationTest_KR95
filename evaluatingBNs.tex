\documentclass[11pt]{amsart}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{booktabs}
\usepackage{wasysym}
                  % ... or a4paper or a5paper or ... 
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{color, colortbl}
\usepackage{xcolor}
\usepackage{adjustbox}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\definecolor{Bittersweet}{rgb}{1.0, 0.44, 0.37}

\title{How can we assess the accuracy of a Bayesian Network?}
\author{The Author}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle

\section{Preamble - Assumptions}
\begin{enumerate}
\item We can create a multi-agent simulation that represents some simple criminal scenario, a possible theft. The frequency at which events occur in this simulation is in part due to interaction of agents with their surroundings and in another part due to randomly generated numbers that represent internal `mental' factors of the agent. The events in this crime scenario can be subdivided into two classes: hypothesis events, and evidence events.
\item We can create a Bayesian Network that represents this multi-agent simulation that corresponds exactly to the frequencies we have observed in the simulation. We create this network by running the simulation for 2000 times, and storing which events took place in each run. On this data, we used a K2 algorithm to create a Bayesian Network (given certain temporal constraints).
\end{enumerate}


\section{Introduction}
In this section, I have disturbed the CPTs of an ``objective'' Bayesian Network (see preamble). I disturb the CPTs with 3 different processes, that might all loosely correspond to human approaches for reasoning with probabilities. I then present the effect of these disturbances in the CPTs, on the ability of the disturbed BNs to get similar outcomes to the original Bayesian Network. 

 The original BN was created with certain temporal constraints (ordering) with the K2 algorithm, with CPTs that exactly represent the underlying probabilistic structure of the multi-agent simulation that the BN must represent. The same nodes, and the same independence relations were used in the disturbed BNs - so only the CPTs were disturbed, nothing else. This is implausible for real situations, where we wouldn't know the independence relations or even the events that transpired. However, you have to start somewhere...

The BN was disturbed by three different processes:

\begin{enumerate}

\item Straightforwardly losing precision by changing the rounding in the CPTs. Rounding took place from 5 decimals (very precise) to 0 decimals (turn the number to 0 or 1, obviously this resulted in a BN that could not be used (incompatible). This is to see if the float errors/precision errors in BNs matter in case of this simulation.

\item Rounding to arbitrary intervals - instead of rounding to some number of decimals, here the range of probabilities is even more restricted and we can only assign probabilities out of a discrete range: like, [0, 0.2, ..., 0.8, 1], or, in the least precise case [0, 0.5, 1]. This should reflect relatively normal human reasoning - if we don't have exact numbers, we want to estimate to some arbitrary degree. In this case I wanted to see how granular our estimation needs to be in order to come to a similar conclusion as the K2 network. 

\item Process 3 is disturbing the CPTs with random normal noise, representing the effect of confusion about directionality/causality/strength in the network. The standard deviation of the normal noise is increased (mean at 0), to represent greater confusing about precise strength of connections between nodes. Here, in the tables below we look at the \% of times that the disturbed network corresponds with the original network.
\end{enumerate}

\subsection{The Weak and the Strong Interpretation of Bayesian Networks}
There are 2 ways of viewing the use of BNs, which I will call the weak view and the strong view. In the weak view, probabilities are subjective estimations, and furthermore, it doesn't exactly matter what these probabilities are, as long as they help us to reason with the evidence. In this view (see Taroni), BNs can help us to reason with complex structures of evidence and hypotheses, without looking much at the numbers, but more at which hypothesis is supported: does some piece of evidence support some hypothesis in some way, even across many different causal connections? However, the exact strength of the probabilities in the CPT does not matter, its not that relevant. The numbers are only relevant insofar as they can cause us to prefer one hypothesis over another. We only care about the direction that evidence leans towards - towards H1 or towards H0?

In the strong view, BNs are also subjective, but here the numbers do actually matter. We can make precise claims or estimations about probabilities, and the numbers actually represent evidence strength in a relatively precise manner. We cannot only look at the direction - the precision/numbers are informative as well. We need to care about the numbers, not just a binary (supports H1 vs H0), but if we see that support for H1 is 0.80, that means something meaningfully different from support for H1 = 0.99.

Hence, here I investigate this weak vs strong view, by looking at both the `support direction' - what, for each hypothesis node in the original network, is the hypothesis that they support (H1 or H0), and does the disturbed network support that same hypothesis, under all processes of disturbance?
The other thing is the `precision' - to what extent does the disturbed network diverge from the actual probability value of H1 in every node in the network? How big is this disturbance?

By H1 supported, I mean that the probability of H1 is larger than the probability of H0 (so that means that H1 = 0.51 vs H0 = 0.49 is read as `supporting H1').

\subsection{Why not just use sensitivity analysis?}
This is a human approach to things that might plausibly go wrong when assigning probabilities to CPTs. Sensitivity Analysis focuses more on impact of nodes on other nodes, while the three processes above are (plausible) human restrictions on assigning probabilities to CPTs. Sensitivity Analysis could still be performed on the original, or the disturbed networks in order to get insight in how different pieces of evidence play their parts.

\section{Conclusion and Discussion}

\subsection{The weak view}
\begin{enumerate}
\item In the weak view, in the original network, we see that once we find that the object is gone, and the lock is broken, H1 is supported for the thief successfully stealing the object, while with this same evidence, the hypothesis for the object being lost, loses support. Nothing further happens, and adding the rest of the evidence results in continued support for the hypothesis H1 for successful stealing.

\item In the rounded disturbance to the original network, we see that the precision of the CPTs does not matter very much in this simulation - for rounding to 0.00001, 0.0001, 0,001, and 0.01, the support for H1's as evidence is added is the same as in the original network. For 0.1, we see that we still get the H1 conclusion in the end - only a lot later. However, by rounding to 0 decimals - effectively creating a 0-or-1 network, we get the outcome of NA - which means that inconsistent evidence is modelled. Not surprising. Overall, disturbance of rounding is surprisingly not-disturbing: for this network, we only need 0.01 precision to represent the frequencies of the simulation.
\item In the rounded to arbitrary precision disturbance to the original network, in the weak view, we see that the final conclusion (H1 in successful stolen with all the evidence) is still reached at the end, although it seems to be delayed - more evidence is needed to get to the same conclusion. However, we only see the delay, we do not see a change in direction (supporting H1 where the original network would only support H0)! This means that even creating a CPTs as un-nuanced as [0, 0.5, 1], we can still get to the same conclusion as the original network. This seems like a plausible way for humans to reason with BNs - where you can get more precise if you need to. Note that the effectiveness of this view depends a lot on the `pre-assumed' independence relations and node selection.
\item In the normal noise disturbance, we see the same process as in the arbitrary rounding, we still get to the same conclusion as the original network. If there's too much noise (sd of 0.5), we get NA again, as expected from the rounded disturbance results. This implies that if there's some noise in the process of assigning probabilities, it does not have to be a disaster - we can get to the same conclusion as the original network, just perhaps a bit later.

\end{enumerate}

\subsection{The strong view}
\begin{enumerate}
\item In the strong view, in the original network, we see that once we find that the object is gone, and the lock is broken, we get a probability of slightly more than 0.5 in the successful stolen node - which means that H1 is supported in the weak view. Then, adding the next two pieces of evidence doesn't do much, until we get to the camera pieces of evidence, where we identify the suspect, and know that the suspect has certain behaviour (flees when observing another agent). Then, we get to a probability of 1.

\item in the rounded disturbance, we see that the posterior probability given pieces of evidence, is basically the same as in the original network. Rounding doesn't affect anything, even up till 0.1 precision (however rounding to 0 decimals breaks the simulation). So - no float errors.
\item In arbitrary rounding, we see a larger difference in the posterior from the original network, but it still has the same shape - so we can reason with evidence even at reduced precision.
\item In the normal noise disturbance, we see that noise can really mess up the shape of the posterior given evidence curve - for large sd, we see that the original shape is not preserved. However, for small sd, we have the same shape, so a bit of noise in the CPT estimation is not a big deal? However, we do see that the final probabilities here are not always 1 at the end, there's some divergence here - how this would fare in an uncertain or more complex situation is hard to know, but I doubt that it will be good.


\end{enumerate}

In conclusion, we see that the network conclusions are relatively robust against changes in CPTs. Especially if we take the weak view, BNs might be useful in making effects of evidence on hypotheses (especially hypotheses that are far away), visible. However, I would not trust a judge that based themselves on exact probability estimates - while the general trends in posteriors are clear, I don't think with the disturbances, we could get to a 0.95 conviction threshold...

However, process 2 of arbitrary rounding is promising for building BNs in collaboration with human workers - if we can establish events and relations, perhaps estimating the probabilities does not have to be that precise to get the same change in the posterior given some set of evidence - we can still show evidence strength.


The main conclusion is that my simulation is not interesting enough - ideally I want to test if nuanced cases with contradictory evidence are represented well under disturbance. That's not possible in this simulation, it's very monotonic... :(

Side note: we can also show why you need hypothesis nodes, and can't just connect evidence to the main outcome nodes - robustness decreases (see page 11/section 5)

\clearpage
\section{Results Plots}
 {\tiny Note - there's a slight position dodge in the lines for certain images, otherwise they would overlap. A value of NA in the weak-view plots means that the BN is inconsistent.}
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.15]{images/K2Weak.png}
\label{default}
\end{center}
\end{figure}
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.15]{images/roundedWeak.png}
\label{default}
\end{center}
\end{figure}
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.17]{images/arbitraryRoundedWeak.png}
\label{default}
\end{center}
\end{figure}
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.17]{images/normalNoiseWeak.png}
\label{default}
\end{center}
\end{figure}


\begin{figure}[htbp]
\centering
\begin{center}
\includegraphics[scale=0.17]{images/K2absolute.png}
\label{default}
\end{center}
\end{figure}
\begin{figure}[htbp]
\centering
\begin{center}
\includegraphics[scale=0.17]{images/roundedabsolute.png}
\label{default}
\end{center}
\end{figure}
\begin{figure}[htbp]
\centering
\begin{center}
\includegraphics[scale=0.17]{images/arbitraryRoundedabsolute.png}
\label{default}
\end{center}
\end{figure}
\begin{figure}[htbp]
\centering
\begin{center}
\includegraphics[scale=0.17]{images/normalNoiseabsolute.png}
\label{default}
\end{center}
\end{figure}



\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.17]{images/K2delta.png}
\label{default}
\end{center}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.17]{images/roundeddelta.png}
\label{default}
\end{center}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.17]{images/arbitraryRoundeddelta.png}
\label{default}
\end{center}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.17]{images/normalNoisedelta.png}
\label{default}
\end{center}
\end{figure}


\clearpage

\section{Why we need hypothesis nodes!}
We need hypothesis nodes, since as you can see, when we round to an arbitrary level of precision (quarters, thirds), in a normal network, the differences between the disturbed and the original network, are relatively small ($\leq$ 0.2). 

However, if we leave out the hypothesis nodes, and only connect evidence directly to ``outcome'' nodes, you see that for larger intervals (so quarters, thirds and even seconds), the network gets into impossible states (NA). 

There's less tolerance here with rounding. If we have hypothesis nodes in between, we can still use quarters/thirds etc and get results that line up with the original network, instead of results that go to NA for degrees of precision that are lower than 0.2.

So with more hypothesis nodes you can be less precise in your exact probability assigning while still preserving the correct outcome. Which makes sense because you can condition on more stuff, you can probably make some calculation here but intuitively this makes sense, right - more hypothesis nodes allows for more nuance/larger truth table, and more conditional dependencies / more incoming arcs, so more complex calculation, hence more nuanced probability even if we constrain cpts to intervals.

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.17]{images/arbitraryRoundedWeakSpider.png}
\label{default}
\end{center}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.17]{images/strongarbitraryRoundedspider.png}
\label{default}
\end{center}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.17]{images/arbitraryRoundeddeltaSpider.png}
\label{default}
\end{center}
\end{figure}

\section{TABLES: Effect of disturbing the cpts by removing precision with rounding}
\let\oldcentering\centering
\renewcommand\centering{\tiny\oldcentering}
\input{../simulationTest/texTables/K2BNrounded_collected_tables.tex}
\clearpage
\section{Effect of disturbing the cpts by rounding to an arbitrary fraction}
\input{../simulationTest/texTables/K2BNarbitraryRounded_collected_tables.tex}
\clearpage
\section{Effect of disturbing the cpts with normal noise}
\input{../simulationTest/texTables/K2BNnormalNoise_collected_tables.tex}
\clearpage



\end{document}  